\documentclass[
	12pt,
	a4paper,
	abstract,
	bibliography=totoc,
	chapterprefix,
	headings=openright,
	numbers=endperiod,
	parskip=half,
	twoside,
]{scrreprt}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{libertinus}
\usepackage[varqu,scaled=0.95]{inconsolata}
\usepackage{newtxmath}

\usepackage[english]{babel}

% Use less space for the page number
\usepackage[margin=2.5cm,footskip=36pt]{geometry}
\usepackage{graphicx}
\usepackage[htt]{hyphenat}
\usepackage{listings}
\usepackage{microtype}
\usepackage{subcaption}
% Required for listings's upquote
\usepackage{textcomp}
\usepackage{upquote}
\usepackage{xcolor}

\usepackage[hyphens]{url}
\usepackage[hidelinks,pagebackref]{hyperref}
\usepackage[capitalise,noabbrev]{cleveref}

\usepackage[color=ovgu-orange]{todonotes}

\usepackage{lipsum}

\graphicspath{{./figures/}}

\definecolor{ovgu-blue}{HTML}{0068B4}
\definecolor{ovgu-darkgray}{HTML}{606060}
\definecolor{ovgu-lightgray}{HTML}{C0C0C0}
\definecolor{ovgu-orange}{HTML}{F39100}
\definecolor{ovgu-purple}{HTML}{7A003F}
\definecolor{ovgu-red}{HTML}{D13F58}

\lstset{
	basicstyle=\ttfamily,
	commentstyle=\color{ovgu-darkgray},
	keywordstyle=\color{ovgu-blue},
	numberstyle=\ttfamily\color{ovgu-darkgray},
	stringstyle=\color{ovgu-purple},
	rulecolor=\color{ovgu-lightgray},
	frame=single,
	numbers=left,
	language=C,
	breaklines=true,
	breakatwhitespace=true,
	postbreak=\hbox{$\hookrightarrow$ },
	showlines=true,
	showstringspaces=false,
	upquote=true,
	tabsize=4,
	gobble=0,
	captionpos=b,
	abovecaptionskip=\medskipamount,
}

\renewcommand*{\backref}[1]{}
\renewcommand*{\backrefalt}[4]{%
\ifcase #1%
{\color{ovgu-darkgray}(\color{ovgu-red}Not~cited\color{ovgu-darkgray})}%
\or%
{\color{ovgu-darkgray}(Cited~on~page~#2)}%
\else%
{\color{ovgu-darkgray}(Cited~on~pages~#2)}%
\fi%
}

\titlehead{\centering\includegraphics[width=0.66\textwidth]{OVGU-INF}}

\subject{Bachelor/Master Thesis}
\title{Title}

\author{
Author\\
{\large\href{mailto:christian.grueneberg@st.ovgu.de}{\nolinkurl{christian.grueneberg@st.ovgu.de}}}
}

\date{\today}

\publishers{
First Reviewer:\\
Jun.-Prof. Dr. Michael Kuhn

\medskip

Second Reviewer:\\
Johannes Wünsche

\medskip

Supervisor:\\
Jun.-Prof. Dr. Michael Kuhn
}

\begin{document}

% \frontmatter
\pagenumbering{roman}

\maketitle

\begin{abstract}

% abstract als letztes schreiben, fast kurz die Arbeit zusammen um schnell einen Überblick zu geben
% enthält den wesentlichen Inhalt und die kurz zusammengefasst das Ergebnis der Arbeit
% stellt heraus was der eigene Anteil war/ist
% ca 1 Seite

-> write this last after everything else

\end{abstract}

\tableofcontents

% \mainmatter
\cleardoubleoddpage
\pagenumbering{arabic}

\chapter{Introduction}
\label{cha:introduction}

% Was ist das Problem und warum ist es wichtig.
% stellt heraus warum der Leser sich dafür interessieren sollte
% die Motivation hinter der Arbeit
% beinhaltet auch was ist neu, was habe ich neues zum Thema beigetragen
% was sind die Ergebnisse,
% es wird auch kurz dargestellt wie vorgeganngen wurde
% der Aufbau der Arbeit wird kurz vorgestellt

% possible structure
\section{Overview}

%- general problem for cache replacement\\
%- primary and secondary storage \\
%- memory hierachy\\
%- change due to new trends like cloud storage and increasing cache size\\
%- using of haura storage layer with different cache implementations clock, clock-pro and ml-clock\\
%- tiered storage stack\\
%- describe why using clock based algorithms -> citation in "It's Time to Revisit LRU vs. FIFO"\\

% sollte ich ausfühlricher schreiben vielleicht noch als Diagramm mit rein bringen das cpu Geschwindigkeit sich schneller entwicklet als
% die Geschwindigkeit für Speicherzugriffe 

Caches are widely used in hardware and software systems today like L3 cache before DRAM, file systems, hard disk drives, databases,
web servers and many more.
Caching usually means that a small and fast memory is used before a larger but slower memory.
However, it can also mean that a memory with different persistency is used, for example an SSD before a hard disk or 
memory of the same type but in different locations, e.g. in Content Delivery Networks local nodes are used as a cache for distant back-end nodes.
In all these cases, the goal is to reduce cost, decrease latency, and increase bandwidth.

\begin{table}[ht]
	\centering
	\begin{tabular}{|l|l|l|l|l|}
		\hline
		\textbf{Cache Type} & \textbf{What Cached} & \textbf{Where Cached} & \textbf{Latency} & \textbf{Managed By}\\
		 &  &  & \textbf{in cycles} &  \\
		\hline
		\hline
		Registers & 4-byte word & CPU registers & 0 & Compiler \\
		\hline
		TLB & Address translation & On-Chip TLB & 0 & Hardware \\
		\hline
		L1 cache & 32-byte block & On-Chip L1 & 1 & Hardware \\
		\hline
		L2 cache & 32-byte block & On-Chip L2 & 10 & Hardware \\
		\hline
		Virtual Memory & 4-KB page & Main Memory & 100 & Hardware + OS \\
		\hline
		Buffer Cache & Parts of files & Main Memory & 100 & OS \\
		\hline
		Network buffer cache & Parts of files & Local disk & 10,000,000 & AFS/NFS client \\
		\hline
		Browser cache & Web pages & Local disk & 10,000,000 & Web browser \\
		\hline
		Web cache & Web pages & Remote server disks & 1,000,000,000 & Web proxy server \\
		\hline
	\end{tabular}
	\caption{Caching Hierarchy}
	\label{tab:caching hierarchy}
\end{table}

The table \ref{tab:caching hierarchy} shows a few examples \cite{7569243}.
Caching is used at different level of the memory hierachy with various cach block sizes, in software or with hardware support and for memory with different latencies.
Therefore many different cache replacement algorthms were proposed for different use cases.

% could describe why clock-based algorithm and not list based were considered.


\section{Contribution}
% check if contribution or contributions is right
- new is direct comparison of clock based cache replacement algorithms\\

\section{Outline}
- description of the structure of the following work\\

% 5-10%, ca. 2-5 Seiten 

\chapter{Background}
\label{cha:background}

% darstellen der nötigen Informationen und Arbeiten um die weitere Arbeit verstehen zu können
% nicht zu weit ausschweifen
% 10-20%, ca. 4-10 Seiten

- next chapter gives a short overview of topics necessary to understand the following work\\

\section{Caching problem}

- also locking is often not considered especially for older algorithms\\
- could describe FIFO, LRU and LFU as the main policies for cache replacement\\

\section{$B^{\epsilon}$-tree}

% own section for $B^{\epsilon}$-tree
%The $B^{\epsilon}$-tree is a write-optimized version of the B-tree, which are widely used for filesystems or databases, with better performance for insert, range queries and key-value updates \cite{bender2015introduction}.
%Unlike the B-tree, the $B^{\epsilon}$-tree has a buffer for each node, where changes to the subtree associated with the node are stored and
%only when the buffer overflows, the changes are written to the nodes of the subtree, which reduces the number of write operations and the data fragmantation.

The idee for the $B^{\epsilon}$-tree was introduced by \cite{brodal2003lower} in a study of the tradeoff between insertions and queries for comparison-based external memory dictionaries.

The basis for $B^{\epsilon}$-trees is the B-tree, which has good performance on queries but suffers from poor performance on small writes, as shown in \cite{bender2015introduction}, 
because the entire node must be updated for each small write.
However, if you reduce the node size to optimize small writes, sequential read performance suffers because many smaller nodes have to be fetched from disk instead of fewer but larger ones.

To optimize for both cases, the $B^{\epsilon}$-tree adds a buffer to each internal node of size $B - B^{\epsilon} $ with $ 0 \leq \epsilon < 1$ as shown in figure \ref{fig:structure B-epsilon-tree}.

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.6]{B-epsilon_structure.png}
	\caption{Structure $B^{\epsilon}$-tree from \cite{bender2015introduction}}
		\label{fig:structure B-epsilon-tree}
\end{figure}

As new data is inserted into the $B^{\epsilon}$-tree, the data is written to the buffer of the root node as "insert message".
Only when the root node's buffer is full, a batch of messages is flushed down the tree, preferably to the node with the most pending messages. 
If a node needs to be split, the buffer is also split between the new nodes.
When the insert message arrives at a leaf node, the data is added to the leaf.

This behaviour leads to the better insertion performance then B-trees, because insertion always happens at the root, no searching for the right insertion location necessary and 
the actual write of data happens delayed in batches only when enough changes are accumulated in a buffer.
On deletion a "tombestone message" will be insert into the tree and will be flushed down the tree to a leaf, like an insertion message.

For queries, in the $B^{\epsilon}$-tree, we also need to check the buffer on the way to the leaf nodes to see if there are any relevant messages.
These messages must be processed, but this does not mean that these messages are flushed down immediately, this can happen later.

So the $B^{\epsilon}$-trees achieves similar asymptotic I/O cost for queries but is better for insertions compared to B-trees.

\section{Haura}
%- introduce haura storage stack and explain structure\\
%- short explaination B-epsilon-tree\\
%- task of the cache\\

\todo[inline]{check definition for haura with Johannes}

Haura is a research storage stack written primarily in Rust.
Unlike traditional file systems, haura runs in user space rather than kernel space.
Also, haura uses a key-value and object interface rather than the usual POSIX interface.
It can be used either directly by an application, in which case haura runs in the process context of the application, or
use JULEA as a wrapper for haura.
JULEA then runs the haura instance detached from the user applications, which means that the user application can be terminated or new applications can be created as long as JULEA is running, resulting in more flexibility compared to direct use.

\todo[inline]{insert schemes from haura docu for better understanding}
\todo[inline]{insert link to haura docu as source}
\todo[inline]{epsilon is written diffrently then in original paper}

The development of Haura started to compare $B^{\epsilon}$-tree to ZFS and ext4 filesystem \cite{wiedemann2018modern}, where the haura stack
achieved an improved write performance especially for small random write workloads and better sequential throughput.
Then Haura would be extended by \cite{hoppner2021design} to support multiple stoarge levels
The work of \cite{hoppner2021design} extended Haura with an object storage interface and the support of multiple storage levels while preserving the benefits of the $B^{\epsilon}$-tree.

As pointed out by \cite{wunsche2022data} the advantage of Haura is that all levels which a relevant to implement and optimize a storage stack a combined in a single codebase, so that it is easy change subsystems and test different approaches.
Haura is structured in layers as shown in figure \ref{fig:structure haura}.

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.4]{overview_haura_level.pdf}
	\caption{Structure haura, from \cite{wiedemann2018modern}}
		\label{fig:structure haura}
\end{figure}
\todo[inline]{check were I have to put the source for the figure, in text or under the figure}


At the top is the \emph{Database Layer} which manages multiple datasets and snapshots. Each dataset provide a key-value interface, snapshots only read-only key-value interface. Also for each dataset exists an own $B^{\epsilon}$-tree and additonally a root tree which saves the allocation bitmaps and metadata.

The next layer, the \emph{Tree Layer} manages the $B^{\epsilon}$-trees.
The \emph{database layer} sends messages to the $B^{\epsilon}$ trees consisting of key-value or key-message pairs that the $B^{\epsilon}$ trees process. A message for key-message pair can contain arbitrary data or apply arbitrary code on data.
Key-value pairs are stored in leave nodes and key-message nodes are stored in inner nodes of the tree.
Each node is an object for the \emph{Data Management Layer} and is tracked individually.

The \emph{Data Management Layer} manages objects for the \emph{Tree Layer} and \emph{database layer} which means cache objects in memory, 
track modifications to objects and write-back modified objects.
Main part of the \emph{Data Management Layer} is the Data Management Unit (DMU).
The DMU is shared by all trees and ensures that no irregular state can be reached.
It takes care of critical disk management such as block allocation.
In addition, the DMU manages the cache, which is the main topic of this work.

The \emph{Storage Pool Layer} is an abstraction over the used storage hardware.
Furthermore the storage is dvided in to tiers from \emph{Fastest}, \emph{Fast}, \emph{Slow} to \emph{Slowest}.
The user can decide how the used hardware falls into these tiers. Also, not all tiers need to be used.

The last layer, the \emph{Vdev Layer} is responsible for actually reading and writing data from the disks.
Single disk, multiple disks or RAID-like configurations can be used.

\section{CLOCK}
%- history of clock \\
%- low overhead like FIFO but an LRU aproximation -> citation from "Multics paging experiment..."\\
%- widely used \\
%- already implemented \\
%- used as baseline \\

Clock cache was introduced by F.J.Corbató \cite{corbato1968paging} for Multics operating system.
It is an approximation of the LRU (Least Recently Used) policy, but with a low runtime overhead, comparable to FIFO.

Unlike previous cache replacement algorithms, clock uses a circular list with a "hand" as a pointer to the current entry.
Each cache entry has a reference bit initialized to 0.
On a cache hit the reference bit for an entry is set to 1.
On a cache miss, we need an entry for eviction.
First, we look at the entry to which the hand is pointing. If the reference bit for this entry is 0, we have found the entry to be displaced.
If the reference bit is 1, we set the reference to 0 and move the pointer to the next entry.
These steps are repeated until we find an entry with an unset reference bit.

\todo[inline]{probably better to use a diagram to show how clock is working}

Clock also remove the lock contention problem of LRU.
For LRU each cache hit change the position of an entry in the LRU queue.
So cache hit have to be serialize behind a lock to be used in a parallel system.
Clock doesn't have this problem, because clock don't has to maintain an ordered queue.

Due to its simplicity and good performance in most cases, the clock algorithm is widely used for example in Linux, Windows and NetBSD.

 \todo[inline]{search for software which use clock as example with source}

\section{CLOCK-PRO}
%- improvement of clock algorithm \\
%- should work better in cases clock has poor performance \\
%- scanning or looping over data\\
%- based on LIRS but adoptable not fixed Threshold\\
%- uses reuse distance as metric \\
%- categorizes entries in hot and cold\\
%- also tracks non-resident entry to make better decisions about \\

Although the clock algorithm performs well in a variety of use cases, problems arise with weakly localized access patterns, as described in \cite{jiang2005making}.
Some problems include:
\begin{itemize}
	\item[1.] - access of many rarely used blocks can evict frequently used blocks for example in sequential scans
	\item[2.] - a loop pattern which is slightly larger then the cache 
	\item[3.] - in application with a B-tree index like databases, index blocks should be in cache but data blocks
				evict index blocks because of the shorter access time
\end{itemize}
% has been many alternative cache replacement algorithms LIRS is only one 
% could also mention more
% three categories of improvements
% 1. explicitly provide hints by user or programmer
% 2. detecting the access patterns failing LRU and switch to an other effective replacement strategie
% 3. using deeper history access information -> LIRS falls into this category

These problems are general problems for LRU base replacement algorithms.
An early attempt to overcome these problems was Low Inter-reference Recency Set (LIRS) \cite{10.1145/511399.511340} which uses the inter reference recency, the reuse distance, instead of last recency and divide the blocks in hot, with with low interreference recency and cold, with high interreference recency.
Unlike LRU, LIRS attempts to keep the hot blocks in the cache and remove only the cold blocks, to overcome the weaknesses of LRU in cases of weak locallity.
To determine which blocks are hot or cold, LIRS also tracks metadata information for a set of cache blocks that are not resident in the cache.
LIRS uses a stack that includes hot, cold, and non-resident cold blocks, and an additional list for only resident cold entries.

Clock-Pro algorithm \cite{jiang2005clock} combines the efficiency from clock with the performance improvements from LIRS.
To achieve a better performance clock-pro combines LIRS stack and list in one circular list ordered by the last access.
Hot blocks, with small recencies are at the head of the list and cold blocks with large recencies are at the tail of the list.
After a cold block is insert into the clock, it is in a test period.
If an access during this test period occure, the cold block become a hot block.
On the other hand if the cold block is not re-access during its test period, it will be removed.
It is possible that during the test period that the cold block will be removed from memory and became non-resident, then the meta data still remains in the clock until the cold block runs out of his test period.
So a non-resident cold block can when a re-access occures, became a hot block.
To generate free space only resident cold blocks are evicted.

Clock-pro uses three handles instead of one like clock.
These three handles are used to determine the test-period and find an eviction candidate.
Since the test period, the threshold for an block to switch from cold to hot and also back from hot to cold is based on the distance between the handles the algorithm is adoptable and doesn't need a fixed threshold unlike other algorithms like CAR or 2Q.

\todo[inline]{insert image for clock-pro layout}

The hot-hand marks the tail of the list, which is the hot entrie with the largest recency.
Any hot entry that would pass the hot hand is converted to a cold entry.
The cold-hand points to the last resident cold page and is used to find a replacement candidate for eviction.
The test-hand points to the last cold page in the test period. Any cold entry which passes this hand leaves the test period and any non-resident cold page in the test period that passes this pointer is removed from the clock.

\todo[inline]{make a diagram to show finding of eviction candidate}
\todo[inline]{decide if I want to use block, page or entry, shouldn't mix them up}

To make clock-pro adaptable the ration for hot to cold entries is not fixed, unlike LIRS with has a fixed parameter for this.
Also the ratio influence if clock-pro behaves more like LRU and less like LIRS.
The cold-hand behaves like the hand in clock, unsetting reference bit and find eviction candidate.
If there are no hot entries clock-pro behaves like clock algorithm. 
However in case of where clock algorithm sturggles, loop and scans, clock-pro can achieve better results.

If there are many entries with a small reuse distance (hot entries) and only a few cold entries. A new cold entry is evicted relatively quickly, but since it is still in the testing phase, it can be re-entered as a hot entry after an early re-access.
Hot entries also need more time before they are evicted because they must transform into cold entries before they are evicted.
For every cold page accessed during its test period we increment the capacity for cold pages by 1 and for every cold page which passes his test period without access we decrement the capacity by 1.
This will also adjust the speed  for cold- and hot-hand.

So clock-pro has the advantage of LRU in case of strong locality and better performance like LIRS in case of weak locality.

\section{ML-CLOCK}
%- newest algorithm of this three\\
%- based on single layer perceptron and online learning \\
%- also uses clock approach for efficient access compared to list based\\
%- also tries to minimize write back of entries\\
%- categorizes entries in clean (unchanged) and dirty (modified) entries\\
%- also tries to optimize write back by sort dirty entries by address in storage not by time \\
%- dirty entries can get second chance based on prediction by perceptron\\
%- perceptron lerns if LRU or LFU is more important \\

ML-clock \cite{cho2021ml} follows the recent trend to incorporate machine learning techniques.
what let ML-clock stand out is that it uses single-layer perceptron (SLP) which is a relatively simple ml technique.
But this allows the use of online learning instead of batch training which is used for neural networks and gives better adoptability.
It uses SLP to learn if it should behave more like LRU or LFU and incorporate learned I/O patterns.
Also ML-clock tries to minimize write operation and prioritize the eviction of clean entries instead of dirty entries.

ML-clock tries to reduce the number of access to the underlying storage media and incorporate machine learning techniques.
It starts with clock algorithm as basis. 
Unlike clock, ML-clock uses two hands.
Clean-hand to find eviction candidates which have not been modified and dirty-hand to find a modified eviction candidate.

To train the SLP we need to track the reference bit, reference count, and a timestamp for last access for every entry.
The reference bit is used just like in clock.
Reference count is to track the frequency of access and the timestamp for the recency.
SLP is used to decide which policies under the given circumstances is more suited.

To predict if an entry will be evicted the following equation is used.
The variables $w_d, w_c$ and $w_b$ represents the weight for reuse-distance, reference count and the bias.
The variables $x_d$ and $x_c$ represents the input values for reuse-distance and the reference count.

prediction: \\
\begin{align}
	f_{predict} (x_d, x_c) =
	\begin{cases}
		0, \quad x_d \cdot w_d + x_c \cdot w_c + 1 \cdot w_b < 0 \\
		1, \quad x_d \cdot w_d + x_c \cdot w_c + 1 \cdot w_b \geq 0
	\end{cases}
\end{align}

The variable $x_d$ must be reduced so that it has the same order of magnitude as $x_c$.
A prediction is triggered to find a victim page for eviction or to decide if a learning operation is triggered.

scaling of $x_d$:
\begin{align}
	x_d = \frac{current \  timestamp - timestamp \  of \  the \  last \  access}{size \  of \  circular \  list}
\end{align}
	
The variable $lr$ is the learning rate and controls how much the model change each epoch.
The equations below are used to train the new weights.
The learing operation is used to update the weights.

learning:
\begin{align}
\begin{split}
	w_d \leftarrow w_d + lr \cdot x_d \cdot (v_{expect} - v_{predict})\\
	w_c \leftarrow w_c + lr \cdot x_c \cdot (v_{expect} - v_{predict})\\
	w_b \leftarrow w_b + lr \cdot x_b \cdot (v_{expect} - v_{predict})
\end{split}
\end{align}

The variable $v_{predict}$ is the result of the prediction equations for a possible victim and 
$v_{expect}$ is the correct answer for this prediction.
The term $(v_{expect} - v_{predict})$ defines if the weightes are updated or not.

In addition, ml-clock uses a ghost queue to store the metadata for eliminated entries in order to learn the weights.
The ghost queue can hold at most the same number of entries as the circular list.
Each time a new entry is added to the ghost queue and the ghost queue is full, a learning operation is triggered.
Useful because every evicted entries can be used to calibrate the SLP.

\todo[inline]{insert diagram or pseudocode to show finding of eviction candidate}
\todo[inline]{show table with preference rules for c- and d-candidate}

\chapter{Design and Implementation}
\label{cha:design and implementation}
% Design and Implementation fehlen?! als Kapitel
% 25-35%, ca 10-14 Seiten

-> can start with this chapter when I decide which benchmark and metrics I want to use\\
- fio for synthetic benchmarks -> zipf dis. and sequential should be covered\\
- try to find traces which I can use easily \\
- some articles uses SPEC-Benchmark traces\\

- general problem that we are dealing with variable sized cache entries and also \\
  pinning an entry is possible which increases the difficulty of implementation of most suggested algorithms\\
- most algorithms based an equally sized pages/entries without pinning\\
- ml-clock with extra locking which could lead to problems, clock and clock-pro uses atomics to prevent this


\chapter{Related Work}
\label{cha:related work}
% Related Work fehlt ebennfalls 
%
% es werden Arbeiten presentiert bzw. meine Arbeit mit diesen vergliechen
% was hab ich anders gemacht bzw. wie sind andere Autoren an das Thema ran gegangen
% 5-10%, ca 2-5 Seiten

- FIFO can be Better than LRU: the Power of Lazy Promotion and Quick Demotion\\
 interisting because for large storage FIFO could provide better results and could be 
 easier to implement especially with the abstraction of the DMU 
 
- maybe one or two of the cache algorithm Johannes suggested like FrozenHot, Arc, Cacheus \\
- shepherd-Cache seems to follow the intention of "FIFO can be Better..." and could be a good candidate \\
- CAR do something like ML-CLOCK, decide between recency and frequency but without perceptron \\
- also a CLOCK-PRO+ exists to improve CLOCK-Pro performance \\
- also a lot more ML algorithms exists\\

\chapter{Evaluation}
\label{cha:evaluation}

% detalierte Beschreibung des Testsystems und der Testumgebung, Software Hardware etc.
% soll reproduzierbar sein
% Wie soll man die Daten angeben reicht Diagramme?
% was sind die Kriterien für die Evaluation, Latenz, Cache-misses, Ausführungszeit, ...
% Interpretation der Resultate
% Vorteile und Nachteile darstellen

% 25-35%, ca. 10-14 Seiten

-> start after benchmarks are done

% possible structure
\section{Setup}

\section{Methodology}
- synthetic benchmarks with fio zipf and random, if I get random to work\\
- zip benchmark with linux kernel source files\\
- also try to find a trace with larger files then linux source files\\

\section{Results}


\chapter{Conclusion}
\label{cha:conclusion}
% could be conclusions and future work

-> after evaluation

% 5-10%, 2-5 Seiten


\bibliographystyle{apalike}
\bibliography{thesis}

% \backmatter
% should be around 20 references for bachelor thesis
% avoid to cite web sites


\appendix

\chapter{Appendix}
\label{cha:appendix}

% could also be ad the beginning
% implement acronyms or list of Abbreviations
% implement list of figures, list of tables and listings

\chapter*{}

\section*{Statement of Authorship}

I herewith assure that I wrote the present thesis independently, that the thesis has not been partially or fully submitted as graded academic work and that I have used no other means than the ones indicated.
I have indicated all parts of the work in which sources are used according to their wording or to their meaning.

I am aware of the fact that violations of copyright can lead to injunctive relief and claims for damages of the author as well as a penalty by the law enforcement agency.

\bigskip

Magdeburg, \today

\bigskip
\bigskip

\rule{0.5\textwidth}{0.5pt}\\
\hspace*{0.25em}Signature

\end{document}
